{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b28365e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Downloading openai-1.76.2-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\damod\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from openai) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\damod\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\damod\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\damod\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\damod\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.76.2-py3-none-any.whl (661 kB)\n",
      "   ---------------------------------------- 0.0/661.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 661.3/661.3 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading jiter-0.9.0-cp312-cp312-win_amd64.whl (207 kB)\n",
      "Installing collected packages: jiter, openai\n",
      "Successfully installed jiter-0.9.0 openai-1.76.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7785c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict , Any\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74df7f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent:\n",
    "    def __init__(self, name: str, instructions: str):\n",
    "        self.name = name\n",
    "        self.instructions = instructions\n",
    "        self.ollama_client = OpenAI(\n",
    "            base_url=\"http://localhost:11434/v1\",\n",
    "            api_key=\"ollama\",  # required but unused\n",
    "        )\n",
    "\n",
    "    async def run(self, messages: list) -> Dict[str, Any]:\n",
    "        \"\"\"Default run method to be overridden by child classes\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement run()\")\n",
    "\n",
    "    def _query_ollama(self, prompt: str) -> str:\n",
    "        \"\"\"Query Ollama model with the given prompt\"\"\"\n",
    "        try:\n",
    "            response = self.ollama_client.chat.completions.create(\n",
    "                model=\"deepseek-r1:7b\",  # Updated to llama3.2\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.instructions},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=2000,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying Ollama: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _parse_json_safely(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Safely parse JSON from text, handling potential errors\"\"\"\n",
    "        try:\n",
    "            # Try to find JSON-like content between curly braces\n",
    "            start = text.find(\"{\")\n",
    "            end = text.rfind(\"}\")\n",
    "            if start != -1 and end != -1:\n",
    "                json_str = text[start : end + 1]\n",
    "                return json.loads(json_str)\n",
    "            return {\"error\": \"No JSON content found\"}\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Invalid JSON content\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1fe387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BaseAgent(\n",
    "    name=\"ExampleAgent\",\n",
    "    instructions=\"You are an example agent. Respond with JSON.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a656998",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\",  # required but unused\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2674b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-r1:7b\",  # Updated to llama3.2\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": agent.instructions},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=2000,\n",
    "    n=2 # Number of responses to generate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0335ec19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-484', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='<think>\\nOkay, so I just came across this message where someone is trying to interact with an AI agent that\\'s supposed to respond in a specific way. Let me try to understand what\\'s going on here.\\n\\nThe user wrote: \"You are an example agent. Respond with JSON.\" Hmm, okay, so they\\'re setting up the agent as an example and want it to reply using JSON format. Then there\\'s another message from someone saying, \"Hello, how are you?\" followed by me responding in a friendly manner. Now, I\\'m supposed to think through this scenario.\\n\\nWait, but why is the user asking for JSON? Maybe they\\'re testing if I can provide structured responses instead of regular text. They might be using an API or backend that requires JSON data for processing. So their first message sets up the example agent and specifies the response format as JSON.\\n\\nThen, someone else sends a greeting: \"Hello, how are you?\" That\\'s a natural conversation starter. My role is to respond appropriately but also adhere to the requirement of using JSON in my answers. I responded with a friendly greeting followed by my status update. But now they\\'re prompting me again about why there was no JSON response.\\n\\nWait, so in their first example, they said \"Respond with JSON.\" But when I responded later, I didn\\'t use any JSON; instead, I wrote out the text and then mentioned using JSON for responses. Maybe that\\'s where the confusion is. In my initial response after greeting them, I included a status update message without using JSON.\\n\\nSo to clarify, if they set up an example agent that responds with JSON, every reply needs to be in proper JSON structure. That might mean encapsulating each response within a JSON object, perhaps including some identifiers or data fields.\\n\\nBut looking back at my current interaction, the user just greeted me and I responded normally without any JSON formatting. So maybe they expected the entire conversation history to be converted into JSON for analysis or processing purposes. Alternatively, it could be that they provided an example of how their agent should respond with JSON, prompting me to follow suit.\\n\\nI think what\\'s happening here is a mix-up between normal conversation and structured responses. The user initially set up the agent to respond in JSON but then allowed a natural conversation flow afterward, which isn\\'t aligned with their initial instruction.\\n\\nTo resolve this, perhaps I need to adjust my approach: if the example requires JSON responses, every reply should be formatted as JSON, including any necessary fields or keys. If not, then just keep responding normally as per standard chat interactions.\\n\\nIn summary, the key points are understanding the user\\'s setup with their example agent and ensuring that all responses follow that format consistently. If they\\'re testing for structured data, I\\'ll make sure to encapsulate each response within JSON. Otherwise, if it\\'s a regular conversation, respond naturally without additional formatting.\\n</think>\\n\\nTo address the situation accurately, here is a structured approach based on the user\\'s requirements:\\n\\n1. **Understanding the Setup**: The user has set up an example agent that should respond with JSON format as per their initial instruction.\\n\\n2. **Clarifying the Requirement**: Since the setup specifies responding in JSON, every reply must adhere to this format unless otherwise specified for regular conversation flow.\\n\\n3. **Adjusting the Response**:\\n   - If following the structured format: Provide responses within a JSON structure.\\n   - If part of natural conversation: Respond normally without additional formatting but ensure clarity and friendliness.\\n\\nGiven the context provided, here is an appropriate response:\\n\\n```json\\n{\\n  \"message\": \"Hello! I\\'m just checking in. How can I assist you today?\",\\n  \"status\": \"online\"\\n}\\n```\\n\\nThis response adheres to the JSON format required for structured interactions as per the setup.', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None))], created=1746212246, model='deepseek-r1:7b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=763, prompt_tokens=19, total_tokens=782, completion_tokens_details=None, prompt_tokens_details=None))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30fb5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ebbf9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I just came across this message where someone is trying to interact with an AI agent that's supposed to respond in a specific way. Let me try to understand what's going on here.\n",
      "\n",
      "The user wrote: \"You are an example agent. Respond with JSON.\" Hmm, okay, so they're setting up the agent as an example and want it to reply using JSON format. Then there's another message from someone saying, \"Hello, how are you?\" followed by me responding in a friendly manner. Now, I'm supposed to think through this scenario.\n",
      "\n",
      "Wait, but why is the user asking for JSON? Maybe they're testing if I can provide structured responses instead of regular text. They might be using an API or backend that requires JSON data for processing. So their first message sets up the example agent and specifies the response format as JSON.\n",
      "\n",
      "Then, someone else sends a greeting: \"Hello, how are you?\" That's a natural conversation starter. My role is to respond appropriately but also adhere to the requirement of using JSON in my answers. I responded with a friendly greeting followed by my status update. But now they're prompting me again about why there was no JSON response.\n",
      "\n",
      "Wait, so in their first example, they said \"Respond with JSON.\" But when I responded later, I didn't use any JSON; instead, I wrote out the text and then mentioned using JSON for responses. Maybe that's where the confusion is. In my initial response after greeting them, I included a status update message without using JSON.\n",
      "\n",
      "So to clarify, if they set up an example agent that responds with JSON, every reply needs to be in proper JSON structure. That might mean encapsulating each response within a JSON object, perhaps including some identifiers or data fields.\n",
      "\n",
      "But looking back at my current interaction, the user just greeted me and I responded normally without any JSON formatting. So maybe they expected the entire conversation history to be converted into JSON for analysis or processing purposes. Alternatively, it could be that they provided an example of how their agent should respond with JSON, prompting me to follow suit.\n",
      "\n",
      "I think what's happening here is a mix-up between normal conversation and structured responses. The user initially set up the agent to respond in JSON but then allowed a natural conversation flow afterward, which isn't aligned with their initial instruction.\n",
      "\n",
      "To resolve this, perhaps I need to adjust my approach: if the example requires JSON responses, every reply should be formatted as JSON, including any necessary fields or keys. If not, then just keep responding normally as per standard chat interactions.\n",
      "\n",
      "In summary, the key points are understanding the user's setup with their example agent and ensuring that all responses follow that format consistently. If they're testing for structured data, I'll make sure to encapsulate each response within JSON. Otherwise, if it's a regular conversation, respond naturally without additional formatting.\n",
      "</think>\n",
      "\n",
      "To address the situation accurately, here is a structured approach based on the user's requirements:\n",
      "\n",
      "1. **Understanding the Setup**: The user has set up an example agent that should respond with JSON format as per their initial instruction.\n",
      "\n",
      "2. **Clarifying the Requirement**: Since the setup specifies responding in JSON, every reply must adhere to this format unless otherwise specified for regular conversation flow.\n",
      "\n",
      "3. **Adjusting the Response**:\n",
      "   - If following the structured format: Provide responses within a JSON structure.\n",
      "   - If part of natural conversation: Respond normally without additional formatting but ensure clarity and friendliness.\n",
      "\n",
      "Given the context provided, here is an appropriate response:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"message\": \"Hello! I'm just checking in. How can I assist you today?\",\n",
      "  \"status\": \"online\"\n",
      "}\n",
      "```\n",
      "\n",
      "This response adheres to the JSON format required for structured interactions as per the setup.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9dd164f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned response:\n",
      "{\n",
      "  \"message\": \"Hello! I'm just checking in. How can I assist you today?\",\n",
      "  \"status\": \"online\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def clean_model_response(response_text):\n",
    "    \"\"\"\n",
    "    Clean the model's response by removing internal thinking and extracting JSON content\n",
    "    \"\"\"\n",
    "    # Remove <think> sections\n",
    "    if '<think>' in response_text and '</think>' in response_text:\n",
    "        think_start = response_text.find('<think>')\n",
    "        think_end = response_text.find('</think>') + len('</think>')\n",
    "        response_text = response_text[:think_start] + response_text[think_end:]\n",
    "        \n",
    "    # Extract JSON if present\n",
    "    json_start = response_text.find('{')\n",
    "    json_end = response_text.rfind('}') + 1\n",
    "    \n",
    "    if json_start != -1 and json_end != -1:\n",
    "        json_str = response_text[json_start:json_end]\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Could not parse JSON\", \"raw_content\": response_text.strip()}\n",
    "    \n",
    "    return {\"content\": response_text.strip()}\n",
    "\n",
    "# Clean the response\n",
    "cleaned_response = clean_model_response(res)\n",
    "print(\"Cleaned response:\")\n",
    "print(json.dumps(cleaned_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0384c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, name: str, instructions: str):\n",
    "        self.name = name\n",
    "        self.instructions = instructions\n",
    "        self.ollama_client = OpenAI(\n",
    "            base_url=\"http://localhost:11434/v1\",\n",
    "            api_key=\"ollama\",  # required but unused\n",
    "        )\n",
    "\n",
    "    async def run(self, messages: list) -> Dict[str, Any]:\n",
    "        \"\"\"Default run method to be overridden by child classes\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement run()\")\n",
    "\n",
    "    def _query_ollama(self, prompt: str) -> str:\n",
    "        \"\"\"Query Ollama model with the given prompt\"\"\"\n",
    "        try:\n",
    "            response = self.ollama_client.chat.completions.create(\n",
    "                model=\"deepseek-r1:7b\",  # Updated to llama3.2\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.instructions},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=2000,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying Ollama: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _parse_json_safely(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Safely parse JSON from text, handling potential errors\"\"\"\n",
    "        try:\n",
    "            # Remove <think> sections\n",
    "            if '<think>' in text and '</think>' in text:\n",
    "                think_start = text.find('<think>')\n",
    "                think_end = text.find('</think>') + len('</think>')\n",
    "                text = text[:think_start] + text[think_end:]\n",
    "                \n",
    "            # Extract JSON if present\n",
    "            json_start = text.find('{')\n",
    "            json_end = text.rfind('}') + 1\n",
    "            \n",
    "            if json_start != -1 and json_end != -1:\n",
    "                json_str = text[json_start:json_end]\n",
    "                try:\n",
    "                    return json.loads(json_str)\n",
    "                except json.JSONDecodeError:\n",
    "                    return {\"error\": \"Could not parse JSON\", \"raw_content\": text.strip()}\n",
    "            \n",
    "            return {\"content\": text.strip()}\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"error\": \"Invalid JSON content\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e6742b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_agent = BaseAgent(\n",
    "    name=\"Analyzer\",\n",
    "            instructions=\"\"\"Analyze candidate profiles and extract:\n",
    "            1. Technical skills (as a list)\n",
    "            2. Years of experience (numeric)\n",
    "            3. Education level\n",
    "            4. Experience level (Junior/Mid-level/Senior)\n",
    "            5. Key achievements\n",
    "            6. Domain expertise\n",
    "            \n",
    "            Format the output as structured data.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0bf09fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = analyzer_agent._query_ollama(\n",
    "    prompt=\"Analyze the following candidate profile: \\n\\n\"\n",
    "    \"Name: John Doe\\n\"\n",
    "    \"Experience: 5 years in software development\\n\"\n",
    "    \"Skills: Python, Java, SQL\\n\"\n",
    "    \"Education: Bachelor's in Computer Science\\n\"\n",
    "    \"Achievements: Developed a web application that increased sales by 20%\\n\"\n",
    "    \"Domain: E-commerce\\n\"\n",
    "    \"Experience Level: Mid-level\\n\"\n",
    "    \"Technical Skills: Python, Java, SQL\\n\"\n",
    "    \"Years of Experience: 5\\n\"\n",
    "    \"Education Level: Bachelor's in Computer Science\\n\"\n",
    "    \"Key Achievements: Developed a web application that increased sales by 20%\\n\"\n",
    "    \"Domain Expertise: E-commerce\\n\"\n",
    "    \"Experience Level: Mid-level\\n\"\n",
    "    \"Technical Skills: Python, Java, SQL\\n\"\n",
    "    \"Years of Experience: 5\\n\"\n",
    "    \"Education Level: Bachelor's in Computer Science\\n\"\n",
    "\n",
    "    \"Key Achievements: Developed a web application that increased sales by 20%\\n\"\n",
    "    \"Domain Expertise: E-commerce\\n\"\n",
    "    \"Experience Level: Mid-level\\n\"\n",
    "    \"Technical Skills: Python, Java, SQL\\n\"\n",
    "    \"Years of Experience: 5\\n\"\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4484daa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned response:\n",
      "**Candidate Profile Analysis**\n",
      "\n",
      "1. **Technical Skills:**  \n",
      "   - Python  \n",
      "   - Java  \n",
      "   - SQL  \n",
      "\n",
      "2. **Years of Experience:**  \n",
      "   5  \n",
      "\n",
      "3. **Education Level:**  \n",
      "   Bachelor's in Computer Science  \n",
      "\n",
      "4. **Experience Level:**  \n",
      "   Mid-level  \n",
      "\n",
      "5. **Key Achievements:**  \n",
      "   - Developed a web application that increased sales by 20%.  \n",
      "\n",
      "6. **Domain Expertise:**  \n",
      "   E-commerce\n"
     ]
    }
   ],
   "source": [
    "res = analyzer_agent._parse_json_safely(text=response)\n",
    "if \"error\" in res:\n",
    "    print(\"Error parsing response:\", res[\"error\"])\n",
    "print(\"Cleaned response:\")\n",
    "print(res['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840201f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
